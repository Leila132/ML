1. Регрессия
Построить регрессию используя аналитическое решение через псевдообратную матрицу Мура-Пенроза. Данные задать следующим образом:

N=1000
x = np.linspace(0, 1, N)
z = 20*np.sin(2*np.pi * 3 * x) + 100*np.exp(x)
error = 10 * np.random.randn(N)
t = z + error

В качестве базисных функций использовать полиномы до степени M.

- В одном графическом окне построить график функции z(x) в виде непрерывной кривой, t(x) в виде точек и решения задачи регрессии в виде непрерывной кривой для M = 1
- В одном графическом окне построить график функции z(x) в виде непрерывной кривой, t(x) в виде точек и решения задачи регрессии в виде непрерывной кривой для M = 8
- В одном графическом окне построить график функции z(x) в виде непрерывной кривой, t(x) в виде точек и решения задачи регрессии в виде непрерывной кривой для M = 100
- Построить график зависимости ошибки E(w) от степени полинома M. M меняется от 1 до 100.


2. Регрессия с регуляризацией. Валидация параметров.
Используя данные из задания 1 построить регрессию с регуляризацией.

N=1000
x = np.linspace(0, 1, N)
z = 20*np.sin(2*np.pi * 3 * x) + 100*np.exp(x)
error = 10 * np.random.randn(N)
t = z + error

В качестве возможных базисных функций использовать полиномы, sin, cos, exp, sqrt (либо другие функции). В качестве возможных значений коэффициента регуляризации задать ограниченный набор значений (например lambda = {0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000}). Для определения лучших параметров модели использовать итеративную рандомизированную процедуру (Монте-Карло) с разделением исходных данных на train, validation, test части . Вычисление параметров модели производить на train части, валидацию параметров на validation части. На лучших подобранных параметрах вычислить ошибку на test части. Значение ошибки на test части, лучшего коэффициента регуляризации и набор лучших базисных функций вывести на консоль.
В одном графическом окне построить график функции z(x) в виде непрерывной кривой, t(x) в виде точек и график регрессии в виде непрерывной кривой, полученной на лучших параметрах модели.

3. Решение регрессии с помощью градиентного спуска.

Скачать один из датасетов на выбор:
1. Boston house price из модуля sklearn.datasets используя функцию load_boston()
2. California Housing из модуля sklearn.datasets используя функцию fetch_california_housing()

Разделить исходный датасет на обучающую и тестовую выборки. На обучающей выборке произвести обучение модели используя градиентный спуск. Базисные функции и коэффициент регуляризации выбрать самим. Начальное приближение в градиентном спуске инициализировать случайными значениями из нормального распределения с нулевым средним и небольшим стандартным отклонением (~0.1). Шаг (learning rate) в градиентном спуске взять ~0.01. Предусмотреть различные критерии остановки градиентного спуска (количество итераций, значение нормы градиента, нормы разности последовательных приближений). Перед началом обучения произвести стандартизацию данных.
Отобразить график зависимости ошибки на обучающей выборке от номера итерации. Для обученной модели вывести на консоль значение ошибки на обучающей и тестовой выборках.

Бонусные задания:
1. Произвести валидацию гиперпараметров модели с базисными функцями в виде полиномов. Сравнить полученную модель с базовым решением, где каждая базисная функция является координатой вектора характеристик.


4. Простые классификаторы и метрики.

Построить данные, моделирующие измерение роста футболистов и баскетболистов (по 1000 на класс). Рост футболистов задать как нормальное распределение со средним mu_0 и стандартным отклонением sigma_0. Рост баскетболистов задать как нормальное распределение со средним mu_1 и стандартным отклонением sigma_1. Задать бинарный классификатор на основе порога (некоторое число). С помощью классификатора произвести классификацию спортсменов по их росту. Реализовать функции, которые по результатам классификации вычисляют метрики: TP, TN, FP, FN, Accuracy, Precision, Recall, F1-score, ошибки 1-го и 2-го рода (alpha, beta).

Постоить график ROC кривой для указанного классификатора:
- Изменять порог от 0 до T
- Найти площадь под построенной кривой (AUC) и вывести её на консоль
- Найти значение порога, при котором достигается максимальное значение Accuracy и подсчитать для него основные метрики: TP, TN, FP, FN, Accuracy, Precision, Recall, F1-score, ошибки 1-го и 2-го рода (alpha, beta). 

5. Логиcтическая регрессия для K классов (Softmax регрессия)

Реализовать модель логистической регрессии для K классов и обучить на датасете digits. Перед обучением произвести стандартизацию векторов характеристик и перевод меток классов в представление one-hot encoding. Перемешать исходный датасет и разделить на обучающую, валидационную выборки. Обучение произвести на обучающей выборке методом градиентного спуска, предварительно инициализировав обучаемые параметры случайным образом. Во время обучения раз в несколько итераций выводить на консоль значения целевой функции и точности (Accuracy) для обучающей и валидационной выборок. Отобразить графики зависимости целевой функции от номера итерации, точности от номера итерации для обучающей и валидационной выборок. Вывести значение точности (Accuracy) модели на валидационной выборке после обучения.


Бонусные задания (необязательные):

1. Предусмотреть реализацию в виде класса с отдельными методами на обучение и предсказание
2. Предусмотреть использование регуляризации:
	- weight decay (сумма квадратов обучаемых параметров)
	- dropout
	- labels smoothing
3. Реализовать градиентный спуск на минибатчах (градиент вычисляется только на части выборки фиксированного размера).
4. Предусмотреть различные инициализации градиентного спуска (равномерное распределение, нормальное, инициализация Xavier, инициализация He)
5. Предусмотреть различные критерии остановки в градиентном спуске
6. Предусмотреть сохранение и считывание модели (например с использованием модуля pickle)
7. Вывести confusion matrix и значения Accuracy для валидационнной выборки до и после обучения
8. Вывести 3 изображения из валидационнной выборки на которых классификатор дал самые уверенные правильные предсказания и 3 изображения на которых классификатор дал самые уверенные неправильные предсказания после обучения.
9. Реализовать отображение графика  целевой функции и графика точности с периодическим обновлением во время обучения.
10. Реализовать модель логистической регрессии для K классов с использованием базисных функций.

6. Классификатор Deсision Tree (Дерево решений).

Реализовать и обучить классифкатор Deсision Tree. Обучение и тестирование произвести на датасете digits из библиотеки sklearn. Классификатор представить в виде бинарного дерева. В качестве критерия качества разделения использовать Information Gain (прирост информации), вычисленный на основе энтропии. В качестве разделяющих функций использовать разделение гиперплоскостями, параллельными осям координат. Обучение дерева произвести с помощью полного перебора. Терминальный узел дерева создавать при выполнении хотя бы одного из трех условий:
- энтропия в узле меньше определенного порога
- глубина дерева в текущем узле дистигла максимального значения
- количество элементов обучающей выборки, достигших узла меньше определенного порога
Вычислить точность (Accuracy) и построить confusion matrix полученной модели на обучающей и тестовой выборках. Построить гистограммы уверенностей правильно распознанных объектов и ошибочных.


Бонусные задания (необязательные):

1. Вместо энтропии в  критерия качества разделения использовать неопределенность Джини (Gini): 
H(S_i) = 1 - SUM (N_i^K/N_i)^2

N_i - множество элементов достигших i узла
N_i^K - множество элементов достигших i узла и принадлежащих классу K

2. Вместо энтропии в  критерия качества разделения использовать ошибку классификации (misclassification error): 
H(S_i) = 1 - max_K (N_i^K/N_i)

N_i - множество элементов достигших i узла
N_i^K - множество элементов достигших i узла и принадлежащих классу K

3. Произвести валидацию гиперпараметров с помощью рандомизированной процедуры:
	- максимальная глубина деревьев
	- критерий (энтропия/неопределенности Джини/ошибку классификации)
	- минимальное значение критерия
	- минимальное количество элементов в терминальных узлах

После валидации вычислить точность и confusion matrix на лучших параметрах.

4. Реализовать вычисление оптимальных параметров разделяющей функции на основе ограниченного перебора этих параметров.

5. В качестве разделяющих функций использовать разделение гиперплоскостями не параллельными осям координат.

6. В качестве разделяющих функций использовать разделение гиперповерхностями, полученными с помощью базисных функций phi (нелинейное разделение).



Дополнительная информация по заданиям:

5. Логиcтическая регрессия для K классов (Softmax регрессия)

Модель:

	Модель классификатора представлена следующей формулой:

	y = Softmax(Wx + b)

	x - вектор характеристик объекта (размер Dx1)
	W - матрица весов (KxD)
	b - вектор смещений (Kx1)
	y - вектор уверенностей классификатора  (Kx1), принадлежности объекта соответствующему классу

	Softmax функция, определенная следующим образом

	y = Softmax(a)

	y_j = exp(a_j)/Sum(exp(a_i))

	Целевая функция для введенного классификатора определяется через кросс-энтропию:

	E(W,b) = SUM SUM t_ik ln y_k(x_i,W,b)

	t - вектор метки представленный с помощью one-hot encoding (вектор стоящий из нулей кроме одной позиции, где стоит 1. позиция совпадает с номером класса, которому принадлежит вектор характеристик).

	Градиент целевой функции по элементам матрицы W и вектора b (одна из возможных записей (матричный вид))

	nabla_W E = (Y-T).T*X
	nabla_b E = (Y-T).T



Датасет:

	Датасет представляет собой изображения рукописных цифр (1797 экземпляров) и находится в библиотеке sklearn. Импорт датасета осуществляется следующим образом
	from sklearn.datasets import load_digits

	digits = load_digits()
	digits.data - вектора характеристик (64 элемента), полученные из изображений цифр путем выписывания в одну строку
	digits.images - изображения цифр 8x8
	digits.target - метки изображений, соответствуют цифре на картинке
	digits.target_names - возможные метки, присутствующие в датасете (цифры от 0 до 9)

	Обращение к элементам датасета возможно и по строковым ключам: digits['data'], digits['images'], digits['target'], digits['target_names']


Предобработка данных:

	Перед обучением модели необходимо произвести предобработку данных - сделать стандратизацию. Эта процедура предполагает вычисление вектора средних значений и вектора стандартных отклонений от векторов характеристик:
	mu = 1/N SUM x_i
	sigma = sqrt(1/N SUM (x_i - mu)^2)

	Далее из каждого вектора характеристик вычитается вектор средних значений и полученный вектор поэлементно делится на вектор стандартных отклонений. Полученные векторы используют для обучения модели.


Обучение модели (Градиентный спуск):

	Обучение модели (подбор оптимальных значений для W и b) осуществляется с помощью градиентного спуска. 
	Общая схема градиентного спуска выглядит следующим образом:

	w_0 = init()
	while some_criteria_to_stop
		w_{k+1} = w_k - gamma*nabla E(w_k)

	Возможные критерии остановки:
	- количество итераций
	- норма значений между последовательными приближениями меньше определенного значения
	- норма градиента меньше определенного значения
	- значение целевой функции на валидационнной выборке имеем тренд к росту
	- ручная остановка

	Инициализация:
	Возможные инициализации W и b:
	- равномерное распределение на небольшом интервале около нуля U(-epsilon, epsilon)
	- нормальное распределение с нулевым средним и небольшой дисперсией N(0, sigma)
	- Xavier
	- He



Создание класса:

	Пример создания простого класса приведен ниже.

	class Circle:

		def __init__(cx,cy, r):
			self.center_x = cx
			self.center_y = cy
			self.radius = r

		def get_area(self):
			a = np.pi*self.radis**2
			return a

		def get_perimeter(self):
			p = 2*np.pi*self.radius
			return p

	с = Circle(5,5,3)
	print(c.area())
	print(c.perimeter())



Вычисления:

	Для избежания переполнений (overflow, underflow) при вычислении функций softmax и ln воспользоваться следующим:
	1. Для softmax умножить числитель и знаменатель на значение exp(r), где r = - max(a_j)
	2. Для ln раскрыть выражение под логарифмом и воспользовавшись правилами ln(ab) = ln(a) + ln(b), ln(a/b) = ln(a) - ln(b) упростить вычисляемое значение


Сохранение модели:

	Сохранение и считывание модели из бинарного файла с помощью модуля pickle. Сохранение можно производить периодически в течении обучения (например, раз в 100 итераций).

	import pickle

	classifier # это обученная модель

	#сохранение модели:

	with open('/path/to/folder/model.pickle', 'wb') as f
		pickle.dump(classifier, f)
		
	#считывание модели
	with open('/path/to/folder/model.pickle', 'rb') as f
		classifier = pickle.load(f)
	
	


Использование базисных функций:

	Построить модель логистической регреcсии на К классов с использованием базисных функций. Модель принимает следующий вид:

	y = Softmax(W*phi(x) + b)

	где phi(x) - вектор значений базисных функций (размер Mx1). Базисные функции выбрать самим. 
	Необходимо пересчитать градиенты с учетом введенных базисных функций.
	Предполагается, что введение базисных функций потенциально может дать прирост точности.
